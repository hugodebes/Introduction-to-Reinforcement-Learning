import random
import numpy as np


class PrioritizedReplayBuffer:
    # Prioritzed Replay Buffer Implementation (Intelligent sampling on the memory)
    def __init__(self, capacity, alpha):
        """
        Create the Buffer

        Arguments
        ---------
            capacity: int
                Size of the buffer (must be a power of 2)
            alpha: float
                HyperParameter to choose how much prioritization is used
        """
        assert capacity % 2 == 0

        self.capacity = capacity
        self.alpha = alpha

        self.priority_sum = [0 for _ in range(2 * self.capacity)]
        self.priority_min = [np.inf for _ in range(2 * self.capacity)]
        self.priority_max = 1.0

        self.data = {
            "obs": np.zeros(shape=(self.capacity, 4, 84, 84), dtype=np.uint8),
            "action": np.zeros(shape=self.capacity, dtype=np.uint32),
            "reward": np.zeros(shape=capacity, dtype=np.uint32),
            "next_obs": np.zeros(shape=(self.capacity, 4, 84, 84), dtype=np.uint8),
            "done": np.zeros(shape=self.capacity, dtype=np.bool),
        }

        self.next_idx = 0
        self.size_buffer = 0

    def add(self, obs, action, reward, next_obs, done):
        """
        Add a new sample to the queue

        Arguments
        ---------
            obs: np.array
                Observation of the current state (frames)
            action: int
                Action taken by the agent
            reward:
                Reward generated by the action taken by the agent
            next_obs: np.array
                New frames after the action
            done: bool
                Is the episode done ?
        """
        # Assign the index of the sample
        idx = self.next_idx
        # Store the new sample
        self.data["obs"][idx] = obs
        self.data["action"][idx] = action
        self.data["reward"][idx] = reward
        self.data["next_obs"][idx] = next_obs
        self.data["done"][idx] = done

        # Update the index (cyclic buffer)
        self.next_idx = (idx + 1) % self.capacity
        # Update the size
        self.size_buffer = min(self.capacity, self.size_buffer + 1)

        # Assign max priority to new sample (num of proba)
        priority_alpha = self.priority_max**self.alpha
        # Update our trees

        # self.priority_sum[idx] = priority_alpha
        # self.priority_min[idx] = priority_alpha

    def priority_segment_tree_min(self, idx, priority_alpha):
        """
        Set the priority
        """
